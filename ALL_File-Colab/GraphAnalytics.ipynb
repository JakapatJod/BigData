{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "\n",
        "!pip install graphframes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeDVcwv2ON-v",
        "outputId": "9bbc3d3c-db90-4aaf-dc81-23def19accbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840625 sha256=c50c8fb78284662bd9ea755927363962afb4488133668ffb5402703eecf26e51\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/3a/92/28b93e2fbfdbb07509ca4d6f50c5e407f48dce4ddbda69a4ab\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.3\n",
            "Collecting graphframes\n",
            "  Downloading graphframes-0.6-py2.py3-none-any.whl.metadata (934 bytes)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from graphframes) (1.26.4)\n",
            "Collecting nose (from graphframes)\n",
            "  Downloading nose-1.3.7-py3-none-any.whl.metadata (1.7 kB)\n",
            "Downloading graphframes-0.6-py2.py3-none-any.whl (18 kB)\n",
            "Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nose, graphframes\n",
            "Successfully installed graphframes-0.6 nose-1.3.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTp7GuUhOEak",
        "outputId": "f8a3c190-c885-4687-a908-2e764f737672"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py:168: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grouped and Ordered Edges:\n",
            "+-------+-----+-----+\n",
            "|src    |dst  |count|\n",
            "+-------+-----+-----+\n",
            "|Alice  |Jacob|1    |\n",
            "|Sheldon|Alice|1    |\n",
            "|Emily  |Jacob|1    |\n",
            "|Alice  |Emily|1    |\n",
            "|Alice  |Roy  |1    |\n",
            "|Jacob  |Roy  |1    |\n",
            "|Ryan   |Alice|1    |\n",
            "+-------+-----+-----+\n",
            "\n",
            "Filtered Edges where src or dst is 'Alice':\n",
            "+-------+-----+-----+\n",
            "|src    |dst  |count|\n",
            "+-------+-----+-----+\n",
            "|Alice  |Jacob|1    |\n",
            "|Sheldon|Alice|1    |\n",
            "|Alice  |Emily|1    |\n",
            "|Alice  |Roy  |1    |\n",
            "|Ryan   |Alice|1    |\n",
            "+-------+-----+-----+\n",
            "\n",
            "Subgraph where 'Alice' is involved:\n",
            "+-------+-----+------------+\n",
            "|src    |dst  |relationship|\n",
            "+-------+-----+------------+\n",
            "|Sheldon|Alice|Sister      |\n",
            "|Alice  |Jacob|Husband     |\n",
            "|Ryan   |Alice|Friend      |\n",
            "|Alice  |Emily|Daughter    |\n",
            "|Alice  |Roy  |Son         |\n",
            "+-------+-----+------------+\n",
            "\n",
            "Motifs in the Graph (connections involving Alice):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
            "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------------------+-----------+\n",
            "|a          |ab                      |b          |\n",
            "+-----------+------------------------+-----------+\n",
            "|{Ryan, 49} |{Ryan, Alice, Friend}   |{Alice, 45}|\n",
            "|{Alice, 45}|{Alice, Emily, Daughter}|{Emily, 24}|\n",
            "+-----------+------------------------+-----------+\n",
            "\n",
            "PageRank Results:\n",
            "+-------+---+-------------------+\n",
            "|id     |age|pagerank           |\n",
            "+-------+---+-------------------+\n",
            "|Roy    |21 |1.9089989375092518 |\n",
            "|Jacob  |43 |1.3728466605994618 |\n",
            "|Alice  |45 |1.135192093597289  |\n",
            "|Emily  |24 |0.7420792759997091 |\n",
            "|Sheldon|52 |0.42044151614714403|\n",
            "|Ryan   |49 |0.42044151614714403|\n",
            "+-------+---+-------------------+\n",
            "\n",
            "In-Degree of Each Vertex:\n",
            "+-----+--------+\n",
            "|id   |inDegree|\n",
            "+-----+--------+\n",
            "|Jacob|2       |\n",
            "|Alice|2       |\n",
            "|Roy  |2       |\n",
            "|Emily|1       |\n",
            "+-----+--------+\n",
            "\n",
            "Connected Components:\n",
            "+-------+---+------------+\n",
            "|id     |age|component   |\n",
            "+-------+---+------------+\n",
            "|Alice  |45 |197568495616|\n",
            "|Jacob  |43 |197568495616|\n",
            "|Roy    |21 |197568495616|\n",
            "|Ryan   |49 |197568495616|\n",
            "|Emily  |24 |197568495616|\n",
            "|Sheldon|52 |197568495616|\n",
            "+-------+---+------------+\n",
            "\n",
            "Strongly Connected Components:\n",
            "+-------+---+-------------+\n",
            "|id     |age|component    |\n",
            "+-------+---+-------------+\n",
            "|Roy    |21 |197568495616 |\n",
            "|Jacob  |43 |395136991232 |\n",
            "|Sheldon|52 |730144440320 |\n",
            "|Emily  |24 |1022202216448|\n",
            "|Alice  |45 |1065151889408|\n",
            "|Ryan   |49 |1477468749824|\n",
            "+-------+---+-------------+\n",
            "\n",
            "Breadth-First Search (BFS):\n",
            "+-----------+-----------------+---------+\n",
            "|from       |e0               |to       |\n",
            "+-----------+-----------------+---------+\n",
            "|{Alice, 45}|{Alice, Roy, Son}|{Roy, 21}|\n",
            "+-----------+-----------------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Import Libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from graphframes import GraphFrame\n",
        "from pyspark.sql.functions import desc\n",
        "\n",
        "# Initialize Spark Session with GraphFrames package\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Graph Analytics\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "    .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.0-s_2.12\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Create vertices and edges DataFrames\n",
        "vertices_data = [\n",
        "    (\"Alice\", 45),\n",
        "    (\"Jacob\", 43),\n",
        "    (\"Roy\", 21),\n",
        "    (\"Ryan\", 49),\n",
        "    (\"Emily\", 24),\n",
        "    (\"Sheldon\", 52)\n",
        "]\n",
        "edges_data = [\n",
        "    (\"Sheldon\", \"Alice\", \"Sister\"),\n",
        "    (\"Alice\", \"Jacob\", \"Husband\"),\n",
        "    (\"Emily\", \"Jacob\", \"Father\"),\n",
        "    (\"Ryan\", \"Alice\", \"Friend\"),\n",
        "    (\"Alice\", \"Emily\", \"Daughter\"),\n",
        "    (\"Alice\", \"Roy\", \"Son\"),\n",
        "    (\"Jacob\", \"Roy\", \"Son\")\n",
        "]\n",
        "\n",
        "# Create DataFrames\n",
        "vertices_df = spark.createDataFrame(vertices_data, [\"id\", \"age\"])\n",
        "edges_df = spark.createDataFrame(edges_data, [\"src\", \"dst\", \"relationship\"])\n",
        "\n",
        "# Create GraphFrame\n",
        "graph = GraphFrame(vertices_df, edges_df)\n",
        "\n",
        "# 1. Grouped and Ordered Edges\n",
        "print(\"Grouped and Ordered Edges:\")\n",
        "graph.edges.groupBy(\"src\", \"dst\").count().orderBy(desc(\"count\")).show(truncate=False)\n",
        "\n",
        "# 2. Filtered Edges where src or dst is 'Alice'\n",
        "print(\"Filtered Edges where src or dst is 'Alice':\")\n",
        "graph.edges.where(\"src = 'Alice' OR dst = 'Alice'\").groupBy(\"src\", \"dst\").count().orderBy(desc(\"count\")).show(truncate=False)\n",
        "\n",
        "# 3. Subgraph where 'Alice' is involved\n",
        "print(\"Subgraph where 'Alice' is involved:\")\n",
        "subgraph_edges = graph.edges.where(\"src = 'Alice' OR dst = 'Alice'\")\n",
        "subgraph = GraphFrame(graph.vertices, subgraph_edges)\n",
        "subgraph.edges.show(truncate=False)\n",
        "\n",
        "# 4. Motifs in the Graph (connections involving Alice)\n",
        "print(\"Motifs in the Graph (connections involving Alice):\")\n",
        "motifs = graph.find(\"(a)-[ab]->(b)\")\n",
        "motifs_filtered = motifs.filter(\"ab.relationship = 'Friend' OR ab.relationship = 'Daughter'\")\n",
        "motifs_filtered.show(truncate=False)\n",
        "\n",
        "# 5. PageRank Results\n",
        "print(\"PageRank Results:\")\n",
        "page_rank = graph.pageRank(resetProbability=0.15, maxIter=5)\n",
        "page_rank.vertices.orderBy(desc(\"pagerank\")).show(truncate=False)\n",
        "\n",
        "# 6. In-Degree of Each Vertex\n",
        "print(\"In-Degree of Each Vertex:\")\n",
        "in_degree = graph.inDegrees\n",
        "in_degree.orderBy(desc(\"inDegree\")).show(truncate=False)\n",
        "\n",
        "# 7. Connected Components\n",
        "spark.sparkContext.setCheckpointDir(\"/tmp/checkpoints\")  # Set checkpoint directory\n",
        "print(\"Connected Components:\")\n",
        "connected_components = graph.connectedComponents()\n",
        "connected_components.show(truncate=False)\n",
        "\n",
        "# 8. Strongly Connected Components\n",
        "print(\"Strongly Connected Components:\")\n",
        "strongly_connected_components = graph.stronglyConnectedComponents(maxIter=5)\n",
        "strongly_connected_components.show(truncate=False)\n",
        "\n",
        "# 9. Breadth-First Search (BFS)\n",
        "print(\"Breadth-First Search (BFS):\")\n",
        "bfs_results = graph.bfs(fromExpr=\"id = 'Alice'\", toExpr=\"id = 'Roy'\", maxPathLength=2)\n",
        "bfs_results.show(truncate=False)\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ]
    }
  ]
}